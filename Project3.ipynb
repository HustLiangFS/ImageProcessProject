{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from skimage import io,transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT = \"Datasets/corel_5k/images/\"\n",
    "dirs = [ROOT+i+\"/\" for i in next(os.walk(ROOT))[1]]\n",
    "files = []\n",
    "[files.extend([i+j for j in next(os.walk(i))[2] if \"jpeg\" in j]) for i in dirs]\n",
    "\n",
    "with open(\"Datasets/corel_5k/labels/training_label\") as f:\n",
    "    train_labels = f.readlines()\n",
    "train_labels = [i.split(\" \")[:] for i in train_labels]\n",
    "train_labels = [[int(j) for j in i if j != '' and j != '\\n']for i in train_labels]\n",
    "random.shuffle(train_labels)\n",
    "train_label = train_labels[:4000]\n",
    "val_label = train_labels[4000:]\n",
    "\n",
    "train_label_dict = {}\n",
    "for i in train_label:\n",
    "    train_label_dict[str(i[0])+\".jpeg\"] = i[1:]\n",
    "    \n",
    "val_label_dict = {}\n",
    "for i in val_label:\n",
    "    val_label_dict[str(i[0])+\".jpeg\"] = i[1:]\n",
    "    \n",
    "with open(\"Datasets/corel_5k/labels/test_label\") as f:\n",
    "    test_labels = f.readlines()\n",
    "test_labels = [i.split(\" \")[:] for i in test_labels]\n",
    "test_labels = [[int(j) for j in i if j != '' and j != '\\n']for i in test_labels]\n",
    "test_label_dict = {}\n",
    "for i in test_labels:\n",
    "    test_label_dict[str(i[0])+\".jpeg\"] = i[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = []\n",
    "val_pairs = []\n",
    "test_pairs = []\n",
    "for i in files:\n",
    "    img_name = i.split(\"/\")[-1]\n",
    "    try:\n",
    "        label = train_label_dict[img_name]\n",
    "        for j in label:\n",
    "            train_pairs.append((i, j))\n",
    "    except KeyError:\n",
    "        if img_name in val_label_dict.keys():\n",
    "            val_pairs.append((i, val_label_dict[img_name]))\n",
    "        elif img_name in test_label_dict.keys():\n",
    "            test_pairs.append((i, test_label_dict[img_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COREL_5K(Dataset):\n",
    "    def __init__(self, data, num, train=True):\n",
    "        super(COREL_5K, self).__init__()\n",
    "        self.data = data\n",
    "        self.num = num\n",
    "        self.mean = [24.534070819674163, 25.25100188727893, 21.32722067148645]\n",
    "        self.train = train\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_path, label = self.data[index]\n",
    "        if not self.train:\n",
    "            if len(label) < 5:\n",
    "                label += [375]*(5-len(label))\n",
    "        img = io.imread(data_path)\n",
    "        if img.shape != (192, 128, 3):\n",
    "            img = transform.resize(img, (192, 192))\n",
    "            img = transform.rotate(img, 90)\n",
    "            img = transform.resize(img, (192, 128))\n",
    "        img = np.array(img)\n",
    "        img[:, :, 0] = img[:, :, 0] - self.mean[0]\n",
    "        img[:, :, 1] = img[:, :, 1] - self.mean[1]\n",
    "        img[:, :, 2] = img[:, :, 2] - self.mean[2]\n",
    "        img = img.astype(np.float32)\n",
    "        return img, np.array(label).astype(np.int64) - 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckX(nn.Module):\n",
    "    expansion = 2\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=4):\n",
    "        super(BottleneckX, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False, groups=groups)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 2, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # SE\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_down = nn.Conv2d(\n",
    "            planes * 2, planes // 8, kernel_size=1, bias=False)\n",
    "        self.conv_up = nn.Conv2d(\n",
    "            planes // 8, planes * 2, kernel_size=1, bias=False)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        # Downsample\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out1 = self.global_pool(out)\n",
    "        out1 = self.conv_down(out1)\n",
    "        out1 = self.relu(out1)\n",
    "        out1 = self.conv_up(out1)\n",
    "        out1 = self.sig(out1)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        res = out1 * out + residual\n",
    "        res = self.relu(res)\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "class SEResNeXt(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=375):\n",
    "        self.inplanes = 64\n",
    "        super(SEResNeXt, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 256, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 1024, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d((6, 4))\n",
    "        self.fc = nn.Linear(1024 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_TRAIN = 100\n",
    "NUM_TEST = 20\n",
    "\n",
    "trainDataset = COREL_5K(train_pairs, NUM_TRAIN)\n",
    "train_loader = DataLoader(dataset=trainDataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "valDataset = COREL_5K(val_pairs, NUM_TEST, train=False)\n",
    "val_loader = DataLoader(dataset=valDataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "testDataset = COREL_5K(test_pairs, NUM_TEST, train=False)\n",
    "test_loader = DataLoader(dataset=testDataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "model = SEResNeXt(BottleneckX, [3, 4, 6, 3], num_classes=375)\n",
    "model.cuda()\n",
    "critrien = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                      | 0/500 [00:00<?, ?b/s]/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "                                                  Process Process-3:\n",
      "Process Process-4:\n",
      "Process Process-1:\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-99f31da41c77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "best_acc = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    train_acc = 0\n",
    "    test_acc = 0\n",
    "    for i, (data, label) in tqdm(enumerate(train_loader), total=500, ncols=50, leave=False, unit='b'):\n",
    "        data = Variable(data).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = critrien(output, label)\n",
    "        train_loss += loss.data[0]\n",
    "        _, predict = torch.max(output, 1)\n",
    "        train_acc += (predict == label).sum().data[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    for i, (data, label) in enumerate(val_loader):\n",
    "        data = Variable(data).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        label1 = label.data\n",
    "        output = model(data)\n",
    "        loss = critrien(output, label[:, 0])\n",
    "        test_loss += loss.data[0]\n",
    "        _, predict = torch.max(output, 1)\n",
    "        pred = predict.data\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i] in list(label1[i]):\n",
    "                test_acc += 1\n",
    "    \n",
    "    print('Epoch [%d/%d], Train Loss: %.4f, Train Acc: %.4f, Test Loss: %.4f, Test Acc: %.4f'\n",
    "            %(epoch+1, NUM_EPOCHS, \n",
    "              train_loss / NUM_TRAIN, train_acc / NUM_TRAIN, \n",
    "              test_loss / NUM_TEST, test_acc / NUM_TEST))\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), \"models/SEResNext.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
