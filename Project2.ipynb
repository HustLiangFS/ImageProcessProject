{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(f):\n",
    "    return  pickle.load(f, encoding='latin1')\n",
    "    \n",
    "def load_CIFAR_batch(filename):\n",
    "    \"\"\" load single batch of cifar \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "    \"\"\" load all of cifar \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)    \n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (50000, 32, 32, 3)\n",
      "Y_train: (50000,)\n",
      "X_test: (10000, 32, 32, 3)\n",
      "Y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "xTrain, yTrain, xTest, yTest = load_CIFAR10(\"Datasets/cifar-10-batches-py/\")\n",
    "print(\"X_train:\", xTrain.shape)\n",
    "print(\"Y_train:\", yTrain.shape)\n",
    "print(\"X_test:\", xTest.shape)\n",
    "print(\"Y_test:\", yTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import data, exposure\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "def extract_features(mode, X_train, X_test):\n",
    "    if mode == \"RAW\":\n",
    "        X_train = X_train\n",
    "        X_test = X_test\n",
    "    elif mode == \"HOG\":\n",
    "        X_train = np.array([hog(rgb2gray(image), orientations=8, pixels_per_cell=(16, 16),\n",
    "                   cells_per_block=(1, 1)) for image in X_train])\n",
    "        X_test = np.array([hog(rgb2gray(image), orientations=8, pixels_per_cell=(16, 16),\n",
    "                   cells_per_block=(1, 1)) for image in X_test])\n",
    "    elif mode == \"LBP\":\n",
    "        X_train = np.array([local_binary_pattern(rgb2gray(image),P=1,R=2) for image in X_train])\n",
    "        X_test = np.array([local_binary_pattern(rgb2gray(image),P=1,R=2) for image in X_test])\n",
    "    elif mode == \"SIFT\":\n",
    "        def sift(img):\n",
    "            gray= cv2.cvtColor(img.astype(np.uint8),cv2.COLOR_BGR2GRAY)  \n",
    "            sift = cv2.xfeatures2d.SIFT_create(18)\n",
    "            kp = sift.detect(gray,None)  \n",
    "            des = sift.compute(gray,kp)\n",
    "            return des[-1]\n",
    "        X_train = np.array([sift(image) for image in X_train])\n",
    "        X_test = np.array([sift(image) for image in X_test])\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    return X_train, X_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10(Dataset):\n",
    "    def __init__(self, data, label, data_num):\n",
    "        super(Cifar10, self).__init__()\n",
    "        self.data_num = data_num\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index].astype(np.float32)\n",
    "        label = self.label[index]\n",
    "        return data, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3072, 3072),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        output = self.model(data)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = 50000\n",
    "NUM_TEST = 10000\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "trainDataset = Cifar10(xTrain, yTrain, NUM_TRAIN)\n",
    "testDataset = Cifar10(xTest, yTest, NUM_TEST)\n",
    "trainLoader = DataLoader(dataset=trainDataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "testLoader = DataLoader(dataset=testDataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "model = MLP()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.0798, Train Acc: 0.2038, Test Loss: 0.0076, Test Acc: 0.2803\n",
      "Epoch [2/50], Train Loss: 0.0073, Train Acc: 0.3200, Test Loss: 0.0070, Test Acc: 0.3614\n",
      "Epoch [3/50], Train Loss: 0.0070, Train Acc: 0.3606, Test Loss: 0.0069, Test Acc: 0.3712\n",
      "Epoch [4/50], Train Loss: 0.0067, Train Acc: 0.3821, Test Loss: 0.0065, Test Acc: 0.4099\n",
      "Epoch [5/50], Train Loss: 0.0066, Train Acc: 0.4012, Test Loss: 0.0065, Test Acc: 0.3984\n",
      "Epoch [6/50], Train Loss: 0.0064, Train Acc: 0.4178, Test Loss: 0.0064, Test Acc: 0.4245\n",
      "Epoch [7/50], Train Loss: 0.0063, Train Acc: 0.4270, Test Loss: 0.0061, Test Acc: 0.4427\n",
      "Epoch [8/50], Train Loss: 0.0061, Train Acc: 0.4403, Test Loss: 0.0063, Test Acc: 0.4229\n",
      "Epoch [9/50], Train Loss: 0.0061, Train Acc: 0.4489, Test Loss: 0.0060, Test Acc: 0.4533\n",
      "Epoch [10/50], Train Loss: 0.0060, Train Acc: 0.4529, Test Loss: 0.0061, Test Acc: 0.4394\n",
      "Epoch [11/50], Train Loss: 0.0058, Train Acc: 0.4651, Test Loss: 0.0060, Test Acc: 0.4501\n",
      "Epoch [12/50], Train Loss: 0.0058, Train Acc: 0.4698, Test Loss: 0.0062, Test Acc: 0.4428\n",
      "Epoch [13/50], Train Loss: 0.0057, Train Acc: 0.4830, Test Loss: 0.0060, Test Acc: 0.4587\n",
      "Epoch [14/50], Train Loss: 0.0056, Train Acc: 0.4882, Test Loss: 0.0058, Test Acc: 0.4811\n",
      "Epoch [15/50], Train Loss: 0.0056, Train Acc: 0.4923, Test Loss: 0.0062, Test Acc: 0.4434\n",
      "Epoch [16/50], Train Loss: 0.0055, Train Acc: 0.4986, Test Loss: 0.0059, Test Acc: 0.4585\n",
      "Epoch [17/50], Train Loss: 0.0055, Train Acc: 0.4985, Test Loss: 0.0057, Test Acc: 0.4810\n",
      "Epoch [18/50], Train Loss: 0.0054, Train Acc: 0.5044, Test Loss: 0.0056, Test Acc: 0.4903\n",
      "Epoch [19/50], Train Loss: 0.0053, Train Acc: 0.5115, Test Loss: 0.0056, Test Acc: 0.4978\n",
      "Epoch [20/50], Train Loss: 0.0053, Train Acc: 0.5195, Test Loss: 0.0057, Test Acc: 0.4822\n",
      "Epoch [21/50], Train Loss: 0.0052, Train Acc: 0.5195, Test Loss: 0.0058, Test Acc: 0.4833\n",
      "Epoch [22/50], Train Loss: 0.0052, Train Acc: 0.5241, Test Loss: 0.0056, Test Acc: 0.4945\n",
      "Epoch [23/50], Train Loss: 0.0051, Train Acc: 0.5350, Test Loss: 0.0057, Test Acc: 0.4945\n",
      "Epoch [24/50], Train Loss: 0.0051, Train Acc: 0.5354, Test Loss: 0.0055, Test Acc: 0.5014\n",
      "Epoch [25/50], Train Loss: 0.0050, Train Acc: 0.5382, Test Loss: 0.0055, Test Acc: 0.5126\n",
      "Epoch [26/50], Train Loss: 0.0050, Train Acc: 0.5404, Test Loss: 0.0056, Test Acc: 0.4888\n",
      "Epoch [27/50], Train Loss: 0.0050, Train Acc: 0.5474, Test Loss: 0.0055, Test Acc: 0.5052\n",
      "Epoch [28/50], Train Loss: 0.0049, Train Acc: 0.5484, Test Loss: 0.0055, Test Acc: 0.5072\n",
      "Epoch [29/50], Train Loss: 0.0049, Train Acc: 0.5568, Test Loss: 0.0057, Test Acc: 0.4987\n",
      "Epoch [30/50], Train Loss: 0.0048, Train Acc: 0.5574, Test Loss: 0.0057, Test Acc: 0.4898\n",
      "Epoch [31/50], Train Loss: 0.0048, Train Acc: 0.5583, Test Loss: 0.0057, Test Acc: 0.4937\n",
      "Epoch [32/50], Train Loss: 0.0048, Train Acc: 0.5661, Test Loss: 0.0057, Test Acc: 0.4935\n",
      "Epoch [33/50], Train Loss: 0.0048, Train Acc: 0.5663, Test Loss: 0.0056, Test Acc: 0.5012\n",
      "Epoch [34/50], Train Loss: 0.0047, Train Acc: 0.5687, Test Loss: 0.0057, Test Acc: 0.5009\n",
      "Epoch [35/50], Train Loss: 0.0047, Train Acc: 0.5708, Test Loss: 0.0056, Test Acc: 0.4948\n",
      "Epoch [36/50], Train Loss: 0.0046, Train Acc: 0.5784, Test Loss: 0.0057, Test Acc: 0.5077\n",
      "Epoch [37/50], Train Loss: 0.0046, Train Acc: 0.5791, Test Loss: 0.0059, Test Acc: 0.5048\n",
      "Epoch [38/50], Train Loss: 0.0046, Train Acc: 0.5821, Test Loss: 0.0056, Test Acc: 0.5021\n",
      "Epoch [39/50], Train Loss: 0.0046, Train Acc: 0.5825, Test Loss: 0.0055, Test Acc: 0.5099\n",
      "Epoch [40/50], Train Loss: 0.0045, Train Acc: 0.5870, Test Loss: 0.0056, Test Acc: 0.5011\n",
      "Epoch [41/50], Train Loss: 0.0046, Train Acc: 0.5839, Test Loss: 0.0056, Test Acc: 0.5138\n",
      "Epoch [42/50], Train Loss: 0.0045, Train Acc: 0.5875, Test Loss: 0.0055, Test Acc: 0.5137\n",
      "Epoch [43/50], Train Loss: 0.0044, Train Acc: 0.5930, Test Loss: 0.0055, Test Acc: 0.5168\n",
      "Epoch [44/50], Train Loss: 0.0044, Train Acc: 0.5979, Test Loss: 0.0054, Test Acc: 0.5226\n",
      "Epoch [45/50], Train Loss: 0.0044, Train Acc: 0.5979, Test Loss: 0.0057, Test Acc: 0.5192\n",
      "Epoch [46/50], Train Loss: 0.0043, Train Acc: 0.6020, Test Loss: 0.0060, Test Acc: 0.5000\n",
      "Epoch [47/50], Train Loss: 0.0043, Train Acc: 0.6061, Test Loss: 0.0057, Test Acc: 0.5172\n",
      "Epoch [48/50], Train Loss: 0.0043, Train Acc: 0.6094, Test Loss: 0.0059, Test Acc: 0.5049\n",
      "Epoch [49/50], Train Loss: 0.0044, Train Acc: 0.6021, Test Loss: 0.0060, Test Acc: 0.5030\n",
      "Epoch [50/50], Train Loss: 0.0043, Train Acc: 0.6041, Test Loss: 0.0058, Test Acc: 0.5115\n",
      "CPU times: user 3min 42s, sys: 1min 42s, total: 5min 25s\n",
      "Wall time: 5min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NUM_EPOCHS = 50\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    train_acc = 0\n",
    "    test_acc = 0\n",
    "    for i, (data, label) in enumerate(trainLoader):\n",
    "        data = Variable(data.view(-1, 3072)).cuda()\n",
    "        label = Variable(label.view(-1)).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        _, predict = torch.max(output, 1)\n",
    "        num_correct = (predict == label).sum()\n",
    "        train_acc += num_correct.data[0]\n",
    "        optimizer.step()\n",
    "        \n",
    "    for i, (data, label) in enumerate(testLoader):\n",
    "        data = Variable(data.view(-1, 3072)).cuda()\n",
    "        label = Variable(label.view(-1)).cuda()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        test_loss += loss.data[0]\n",
    "        _, predict = torch.max(output, 1)\n",
    "        num_correct = (predict == label).sum()\n",
    "        test_acc += num_correct.data[0]\n",
    "        \n",
    "    print('Epoch [%d/%d], Train Loss: %.4f, Train Acc: %.4f, Test Loss: %.4f, Test Acc: %.4f'\n",
    "            %(epoch+1, NUM_EPOCHS, \n",
    "              train_loss / NUM_TRAIN, train_acc / NUM_TRAIN, \n",
    "              test_loss / NUM_TEST, test_acc / NUM_TEST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
