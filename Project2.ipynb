{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(f):\n",
    "    return  pickle.load(f, encoding='latin1')\n",
    "    \n",
    "def load_CIFAR_batch(filename):\n",
    "    \"\"\" load single batch of cifar \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "    \"\"\" load all of cifar \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)    \n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (50000, 32, 32, 3)\n",
      "Y_train: (50000,)\n",
      "X_test: (10000, 32, 32, 3)\n",
      "Y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "xTrain, yTrain, xTest, yTest = load_CIFAR10(\"Datasets/cifar-10-batches-py/\")\n",
    "print(\"X_train:\", xTrain.shape)\n",
    "print(\"Y_train:\", yTrain.shape)\n",
    "print(\"X_test:\", xTest.shape)\n",
    "print(\"Y_test:\", yTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10(Dataset):\n",
    "    def __init__(self, data, label, data_num):\n",
    "        super(Cifar10, self).__init__()\n",
    "        self.data_num = data_num\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.MEAN = [125.306918046875, 122.950394140625, 113.86538318359375]\n",
    "        self.STD = [62.993219278136884, 62.08870764001421, 66.70489964063091]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 归一化\n",
    "        data = self.data[index].astype(np.float32)\n",
    "        data[:, :, 0] = (data[:, :, 0] - self.MEAN[0])\n",
    "        data[:, :, 1] = (data[:, :, 1] - self.MEAN[1])\n",
    "        data[:, :, 2] = (data[:, :, 2] - self.MEAN[2])\n",
    "        label = self.label[index]\n",
    "        return data, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3072, 3072),\n",
    "            nn.BatchNorm1d(3072),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        output = self.model(data)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = 50000\n",
    "NUM_TEST = 10000\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "trainDataset = Cifar10(xTrain, yTrain, NUM_TRAIN)\n",
    "testDataset = Cifar10(xTest, yTest, NUM_TEST)\n",
    "trainLoader = DataLoader(dataset=trainDataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "testLoader = DataLoader(dataset=testDataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "model = MLP()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.0069, Train Acc: 0.3709, Test Loss: 0.0060, Test Acc: 0.4540\n",
      "Epoch [2/50], Train Loss: 0.0060, Train Acc: 0.4492, Test Loss: 0.0055, Test Acc: 0.4957\n",
      "Epoch [3/50], Train Loss: 0.0056, Train Acc: 0.4854, Test Loss: 0.0052, Test Acc: 0.5166\n",
      "Epoch [4/50], Train Loss: 0.0053, Train Acc: 0.5086, Test Loss: 0.0051, Test Acc: 0.5290\n",
      "Epoch [5/50], Train Loss: 0.0051, Train Acc: 0.5326, Test Loss: 0.0050, Test Acc: 0.5465\n",
      "Epoch [6/50], Train Loss: 0.0049, Train Acc: 0.5460, Test Loss: 0.0049, Test Acc: 0.5547\n",
      "Epoch [7/50], Train Loss: 0.0048, Train Acc: 0.5641, Test Loss: 0.0048, Test Acc: 0.5572\n",
      "Epoch [8/50], Train Loss: 0.0046, Train Acc: 0.5763, Test Loss: 0.0047, Test Acc: 0.5611\n",
      "Epoch [9/50], Train Loss: 0.0045, Train Acc: 0.5889, Test Loss: 0.0047, Test Acc: 0.5672\n",
      "Epoch [10/50], Train Loss: 0.0044, Train Acc: 0.5991, Test Loss: 0.0047, Test Acc: 0.5709\n",
      "Epoch [11/50], Train Loss: 0.0042, Train Acc: 0.6144, Test Loss: 0.0046, Test Acc: 0.5783\n",
      "Epoch [12/50], Train Loss: 0.0041, Train Acc: 0.6218, Test Loss: 0.0046, Test Acc: 0.5787\n",
      "Epoch [13/50], Train Loss: 0.0040, Train Acc: 0.6331, Test Loss: 0.0046, Test Acc: 0.5804\n",
      "Epoch [14/50], Train Loss: 0.0039, Train Acc: 0.6430, Test Loss: 0.0046, Test Acc: 0.5788\n",
      "Epoch [15/50], Train Loss: 0.0038, Train Acc: 0.6507, Test Loss: 0.0045, Test Acc: 0.5860\n",
      "Epoch [16/50], Train Loss: 0.0037, Train Acc: 0.6616, Test Loss: 0.0045, Test Acc: 0.5902\n",
      "Epoch [17/50], Train Loss: 0.0036, Train Acc: 0.6699, Test Loss: 0.0046, Test Acc: 0.5901\n",
      "Epoch [18/50], Train Loss: 0.0035, Train Acc: 0.6787, Test Loss: 0.0045, Test Acc: 0.5917\n",
      "Epoch [19/50], Train Loss: 0.0034, Train Acc: 0.6883, Test Loss: 0.0045, Test Acc: 0.5983\n",
      "Epoch [20/50], Train Loss: 0.0033, Train Acc: 0.6923, Test Loss: 0.0046, Test Acc: 0.5939\n",
      "Epoch [21/50], Train Loss: 0.0033, Train Acc: 0.6990, Test Loss: 0.0045, Test Acc: 0.5974\n",
      "Epoch [22/50], Train Loss: 0.0032, Train Acc: 0.7072, Test Loss: 0.0045, Test Acc: 0.6023\n",
      "Epoch [23/50], Train Loss: 0.0031, Train Acc: 0.7151, Test Loss: 0.0046, Test Acc: 0.6036\n",
      "Epoch [24/50], Train Loss: 0.0030, Train Acc: 0.7218, Test Loss: 0.0046, Test Acc: 0.5993\n",
      "Epoch [25/50], Train Loss: 0.0030, Train Acc: 0.7290, Test Loss: 0.0047, Test Acc: 0.5973\n",
      "Epoch [26/50], Train Loss: 0.0029, Train Acc: 0.7365, Test Loss: 0.0047, Test Acc: 0.6025\n",
      "Epoch [27/50], Train Loss: 0.0028, Train Acc: 0.7418, Test Loss: 0.0047, Test Acc: 0.6029\n",
      "Epoch [28/50], Train Loss: 0.0028, Train Acc: 0.7462, Test Loss: 0.0046, Test Acc: 0.6075\n",
      "Epoch [29/50], Train Loss: 0.0027, Train Acc: 0.7521, Test Loss: 0.0047, Test Acc: 0.5999\n",
      "Epoch [30/50], Train Loss: 0.0026, Train Acc: 0.7619, Test Loss: 0.0047, Test Acc: 0.6012\n",
      "Epoch [31/50], Train Loss: 0.0025, Train Acc: 0.7692, Test Loss: 0.0048, Test Acc: 0.6056\n",
      "Epoch [32/50], Train Loss: 0.0025, Train Acc: 0.7718, Test Loss: 0.0048, Test Acc: 0.6026\n",
      "Epoch [33/50], Train Loss: 0.0024, Train Acc: 0.7747, Test Loss: 0.0048, Test Acc: 0.6020\n",
      "Epoch [34/50], Train Loss: 0.0024, Train Acc: 0.7811, Test Loss: 0.0048, Test Acc: 0.6058\n",
      "Epoch [35/50], Train Loss: 0.0023, Train Acc: 0.7841, Test Loss: 0.0049, Test Acc: 0.6008\n",
      "Epoch [36/50], Train Loss: 0.0023, Train Acc: 0.7919, Test Loss: 0.0048, Test Acc: 0.6054\n",
      "Epoch [37/50], Train Loss: 0.0022, Train Acc: 0.7935, Test Loss: 0.0049, Test Acc: 0.6045\n",
      "Epoch [38/50], Train Loss: 0.0022, Train Acc: 0.7980, Test Loss: 0.0049, Test Acc: 0.6026\n",
      "Epoch [39/50], Train Loss: 0.0021, Train Acc: 0.8027, Test Loss: 0.0049, Test Acc: 0.6064\n",
      "Epoch [40/50], Train Loss: 0.0021, Train Acc: 0.8053, Test Loss: 0.0049, Test Acc: 0.6020\n",
      "Epoch [41/50], Train Loss: 0.0021, Train Acc: 0.8114, Test Loss: 0.0050, Test Acc: 0.6061\n",
      "Epoch [42/50], Train Loss: 0.0020, Train Acc: 0.8146, Test Loss: 0.0049, Test Acc: 0.6093\n",
      "Epoch [43/50], Train Loss: 0.0020, Train Acc: 0.8180, Test Loss: 0.0050, Test Acc: 0.6033\n",
      "Epoch [44/50], Train Loss: 0.0019, Train Acc: 0.8215, Test Loss: 0.0051, Test Acc: 0.6052\n",
      "Epoch [45/50], Train Loss: 0.0019, Train Acc: 0.8271, Test Loss: 0.0052, Test Acc: 0.6066\n",
      "Epoch [46/50], Train Loss: 0.0019, Train Acc: 0.8302, Test Loss: 0.0051, Test Acc: 0.6089\n",
      "Epoch [47/50], Train Loss: 0.0019, Train Acc: 0.8298, Test Loss: 0.0052, Test Acc: 0.6067\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NUM_EPOCHS = 50\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    train_acc = 0\n",
    "    test_acc = 0\n",
    "    model.train()\n",
    "    for i, (data, label) in enumerate(trainLoader):\n",
    "        data = Variable(data.view(-1, 3072)).cuda()\n",
    "        label = Variable(label.view(-1)).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        _, predict = torch.max(output, 1)\n",
    "        num_correct = (predict == label).sum()\n",
    "        train_acc += num_correct.data[0]\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    for i, (data, label) in enumerate(testLoader):\n",
    "        data = Variable(data.view(-1, 3072)).cuda()\n",
    "        label = Variable(label.view(-1)).cuda()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        test_loss += loss.data[0]\n",
    "        _, predict = torch.max(output, 1)\n",
    "        num_correct = (predict == label).sum()\n",
    "        test_acc += num_correct.data[0]\n",
    "        \n",
    "    print('Epoch [%d/%d], Train Loss: %.4f, Train Acc: %.4f, Test Loss: %.4f, Test Acc: %.4f'\n",
    "            %(epoch+1, NUM_EPOCHS, \n",
    "              train_loss / NUM_TRAIN, train_acc / NUM_TRAIN, \n",
    "              test_loss / NUM_TEST, test_acc / NUM_TEST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP (\n",
       "  (model): Sequential (\n",
       "    (0): Linear (3072 -> 3072)\n",
       "    (1): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU (0.01, inplace)\n",
       "    (3): Dropout (p = 0.5)\n",
       "    (4): Linear (3072 -> 1024)\n",
       "    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (6): LeakyReLU (0.01, inplace)\n",
       "    (7): Dropout (p = 0.5)\n",
       "    (8): Linear (1024 -> 256)\n",
       "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (10): LeakyReLU (0.01, inplace)\n",
       "    (11): Dropout (p = 0.5)\n",
       "    (12): Linear (256 -> 10)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
